{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://archive.ics.uci.edu/dataset/2/adult\n",
    "\n",
    "Predictive Analytics: The Power to Predict Who Will Click, Buy, Lie, or Die\n",
    "by Eric Siegel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests  # for my helper function file\n",
    "from pathlib import Path  # for my helper function file\n",
    "\n",
    "# import the ensemble assignment\n",
    "from sklearn.ensemble import (\n",
    "    BaggingClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    StackingClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "\n",
    "# import models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# try unsupervised learning CANNOT USE BOOSTING OR VOTING ON UNSUPERVISED\n",
    "from sklearn.cluster import (\n",
    "    KMeans,\n",
    ")  # remmeber to use KMeans++ - bagging and stacking allowed\n",
    "from sklearn.cluster import DBSCAN  # totally just extra - bagging and stacking allowed\n",
    "\n",
    "# import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# import pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# import ability to visualize data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import cross validation\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education-num\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital-gain\",\n",
    "    \"capital-loss\",\n",
    "    \"hours-per-week\",\n",
    "    \"native-country\",\n",
    "    \"income\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset with named columns created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the 'adult.data' file with named columns\n",
    "data_df = pd.read_csv(\"adult.data\", names=column_names)\n",
    "\n",
    "# Load the 'adult.test' file with named columns\n",
    "test_df = pd.read_csv(\"adult.test\", names=column_names)\n",
    "# Combine the two DataFrames\n",
    "df = pd.concat([data_df, test_df], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file (optional)\n",
    "df.to_csv(\"adult_combined.csv\", index=False)\n",
    "\n",
    "print(\"Combined dataset with named columns created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode = [\n",
    "    \"workclass\",\n",
    "    \"education\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"native-country\",\n",
    "    \"income\",\n",
    "]\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "for column in columns_to_encode:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"income\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['income'] = df['income'].isin([\"<=50K\", \"<=50K.\", \">50K\", \">50K.\"]).astype(int)\n",
    "# df['income_bin'] = df['income'].map(lambda x: 0 if x in [\"<=50K\", \"<=50K.\"] else 1)\n",
    "# df['income'] = df['income'].astype(str).str.strip().str.lower().map(lambda x: 0 if x == \"<=50K\" else 1)\n",
    "\n",
    "# df['income'] = df['income'].astype(str).str.replace(r'[^\\w\\s<>=]', '', regex=True).str.strip()  # Remove punctuation and extra spaces\n",
    "# df['income_bin'] = np.where(df['income'].isin(['<=50K', '<=50K.']), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data\n",
    "# df_filtered = df[df['native-country'] == \"United-States\"]\n",
    "# df_filtered = df[lambda x: x['native-country'] == \"United-States\"]\n",
    "# # df[\"native-country\"].value_counts()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtered.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corr = df.drop(columns= [\"workclass\", \"education\", \"marital-status\", \"occupation\",\"relationship\", \"race\", \"sex\", \"native-country\", \"income\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation_matrix = df_corr.corr()\n",
    "# correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dummies = pd.get_dummies(df, columns=[\"race\", \"marital-status\"], drop_first=True, dummy_na=True, dtype=bool)\n",
    "\n",
    "# married_statuses = [\n",
    "#     'Married-civ-spouse', 'Married-AF-spouse', 'Married-spouse-absent'\n",
    "# ]\n",
    "# df['marital_status_bin'] = df['marital-status'].apply(lambda x: 1 if x in married_statuses else 0)\n",
    "# df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify boolean columns\n",
    "# bool_cols = df.select_dtypes(include='bool').columns\n",
    "\n",
    "# # Convert boolean columns to integers (0s and 1s)\n",
    "# df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get categorical columns\n",
    "# categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# # Create dummy variables for all categorical columns\n",
    "# df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "minMax_Scaled = MinMaxScaler()\n",
    "\n",
    "X = df.drop(\n",
    "    columns=[\n",
    "        \"workclass\",\n",
    "        \"education\",\n",
    "        \"marital-status\",\n",
    "        \"occupation\",\n",
    "        \"relationship\",\n",
    "        \"race\",\n",
    "        \"sex\",\n",
    "        \"native-country\",\n",
    "    ]\n",
    ")\n",
    "y = df[\"income\"]\n",
    "X = minMax_Scaled.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init models -- Suggest Hyperparameter tuning\n",
    "lr_model = LogisticRegression()  # dan\n",
    "nb_model = GaussianNB()  # julio\n",
    "svc_model = LinearSVC()  # julio\n",
    "rfc_model = RandomForestClassifier()\n",
    "dtc_model = DecisionTreeClassifier()\n",
    "# KNN dan\n",
    "minMax_Scaled = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging\n",
    "bagging_model = BaggingClassifier(estimator=rfc_model, n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "y_pred = bagging_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "bagging_model = BaggingClassifier(estimator=dtc_model, n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "y_pred = bagging_model.predict(X_test)\n",
    "accuracy2 = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfc_model: 1.0\n",
      "dtc_model: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"rfc_model: {accuracy}\")\n",
    "print(f\"dtc_model: {accuracy2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting\n",
    "boosting_model = GradientBoostingClassifier(\n",
    "    n_estimators=100, learning_rate=0.1, random_state=42\n",
    ")\n",
    "boosting_model.fit(X_train, y_train)\n",
    "y_pred = boosting_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Boosting Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking\n",
    "# We use multiple models so add the ones you wantto use in an array first - save the final estimator for the end\n",
    "level1_models = []\n",
    "# Define the final estimator (meta-learner) for the second level\n",
    "final_estimator = (\n",
    "    LogisticRegression()\n",
    ")  # for example. Can use anything else - maybe try some hyperparameter tuning first?\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=level1_models, final_estimator=final_estimator, cv=5\n",
    ")\n",
    "stacking_model.fit(X_train, y_train)\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Stacking Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting / Majority Method\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=level1_models, voting=\"hard\"\n",
    ")  # Hard voting for classification - SOFT is regression\n",
    "voting_model.fit(X_train, y_train)\n",
    "y_pred = voting_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Majority Voting Model Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
